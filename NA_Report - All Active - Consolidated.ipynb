{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Get the previous date\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "#Interacting with Postgres SQL\n",
    "import psycopg2\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#importing database\n",
    "from database_config import postgres as cfg\n",
    "from report_queries import Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the connection to the PostgreSQL -feeddate\n",
    "conn = psycopg2.connect(**cfg)\n",
    "\n",
    "#this works with the last days of the month as well.\n",
    "todays_date = str(datetime.today())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries to Get All active clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_client_query = (\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    name\n",
    "FROM \"public\".\"customers\"\n",
    "WHERE active = 't';\n",
    "\"\"\")\n",
    "\n",
    "active_client = pd.read_sql(active_client_query,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_client = next(iter(active_client.to_dict().values()))\n",
    "\n",
    "rel_query = (\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT ct.customer_id\n",
    "    FROM \"curation_tasks\" AS ct\n",
    "    WHERE started_at >= CURRENT_TIMESTAMP - INTERVAL '1 week'\n",
    "    AND (ct.resolution IS NULL OR ct.resolution = 'misclassified')\"\"\")\n",
    "\n",
    "rel_client = pd.read_sql(rel_query,conn)\n",
    "rel_client_weekly = list(rel_client['customer_id'])\n",
    "\n",
    "active_client = active_client[active_client['id'].isin(rel_client_weekly)]\n",
    "active_client.set_index('id',drop=True,inplace=True)\n",
    "\n",
    "customer_dictionary  = next(iter(active_client.to_dict().values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a list to iterate through whenever completing a job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ids = list(customer_dictionary.keys())\n",
    "customer_names = list(customer_dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 query and data clean completed\n",
      "32 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "52569 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "80 query and data clean completed\n",
      "80 query and data clean completed\n",
      "2325 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "81 query and data clean completed\n",
      "81 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "16690 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "11 query and data clean completed\n",
      "11 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "34808 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "85 query and data clean completed\n",
      "85 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "12548 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "77 query and data clean completed\n",
      "77 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "14094 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "55 query and data clean completed\n",
      "55 query and data clean completed\n",
      "3980 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "71 query and data clean completed\n",
      "71 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "94070 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "83 query and data clean completed\n",
      "83 query and data clean completed\n",
      "876 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "21 query and data clean completed\n",
      "21 query and data clean completed\n",
      "23825 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "13 query and data clean completed\n",
      "13 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "50782 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "39 query and data clean completed\n",
      "39 query and data clean completed\n",
      "100000 Being Processed\n",
      "100000 Being Processed\n",
      "54374 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n",
      "No data available in your time frame for Cust_id  64\n",
      "No data available in your time frame for Cust_id  64\n",
      "No data available for 64\n",
      "9 query and data clean completed\n",
      "9 query and data clean completed\n",
      "83 Being Processed\n",
      "dropping duplicates\n",
      "starting merge\n",
      "on to the next\n"
     ]
    }
   ],
   "source": [
    "#creating specific columns to append to the data\n",
    "agg_data = pd.DataFrame(columns = ['curation_task_id', 'customer_id_x', 'resolution', 'started_at',\n",
    "       'product_id', 'attribute_name', 'attribute_id', 'Total_NAs',\n",
    "       'curation_tasks_count', 'customer_id_y', 'bucket_name', 'product_name',\n",
    "       'active', 'external_id', 'image_url', 'family_friendly'])\n",
    "\n",
    "agg_data_path = 'C:/Users/groupby/Documents/Python Scripts/NA_Reports/data/Weekly_agg_na.csv'\n",
    "agg_data.to_csv(agg_data_path,index=False)\n",
    "\n",
    "for customer in customer_ids:\n",
    "    temp_na = Reports(conn).client_na_report(customer)\n",
    "    print(customer_id, 'query and data clean completed')\n",
    "    temp_na['customer_name'] =  customer_dictionary[customer]\n",
    "    try:\n",
    "        temp_products = Reports(conn).client_products(customer)\n",
    "        print('dropping duplicates')\n",
    "        temp_products.drop_duplicates(inplace=True)\n",
    "        print('starting merge')\n",
    "        agg_data = pd.merge(temp_na,temp_products,on='product_id')\n",
    "        agg_data.to_csv('C:/Users/groupby/Documents/Python Scripts/NA_Reports/data/Weekly_agg_na.csv',mode='a',index=False,header=False)\n",
    "        print('on to the next')\n",
    "    except:\n",
    "        print(\"No data available for \"+str(customer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agg_data.shape[0] > 0:\n",
    "    Gdrive_log_file = \"G:/My Drive/Projects/NA_Reports/Weekly_NA_Report/Weekly_NA_Report_log.txt\"\n",
    "    with open(Gdrive_log_file,'r+',encoding='utf-8') as f:\n",
    "        previous_contents =f.read()\n",
    "        f.seek(0,0)\n",
    "        f.write(todays_date+' update has been confirmed.\\n')\n",
    "        f.write(previous_contents)\n",
    "else:\n",
    "    with open(Gdrive_log_file,'r+',encoding='utf-8') as f:\n",
    "        previous_contents =f.read()\n",
    "        f.seek(0,0)\n",
    "        f.write(todays_date+' an error has occured and no data has been updated.\\n')\n",
    "        f.write(previous_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
